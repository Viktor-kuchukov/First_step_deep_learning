{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n"
     ]
    }
   ],
   "source": [
    "predictions= np.array([-10, 0, 10])\n",
    "probs=np.exp(predictions)/np.sum(np.exp(predictions))\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(probs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.551444713932051 [ 0.57611688 -0.78805844  0.21194156]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print(loss, grad)\n",
    "#Не совсем понятно, как может проверяться градиент функции потерь, если мы вычисляем число в заданной точке.\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя проверка не пройдена, хотя численно вроде всё работает, и градиент и функции потерь сходятся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2. -1.  1.]] \n",
      " [[0.20603191 0.56005279 0.02788339 0.20603191]] \n",
      " [[2]]\n",
      "Gradient check passed!\n",
      "[[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]] \n",
      " [[0.68145256 0.03392753 0.03392753 0.25069239]\n",
      " [0.10923177 0.29692274 0.29692274 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]] \n",
      " [[3]\n",
      " [3]\n",
      " [2]]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print(predictions, \"\\n\", linear_classifer.softmax(predictions),\"\\n\", target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print(predictions, \"\\n\", linear_classifer.softmax(predictions),\"\\n\", target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5797242232074917"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(linear_classifer.softmax(np.array([1, 2, -1, 1]))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20603190920009948"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-np.log(linear_classifer.softmax(np.array([1, 2, -1, 1+delta]))[2])+np.log(linear_classifer.softmax(np.array([1, 2, -1, 1-delta]))[2]))/(2*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38352864, 3.38352864, 3.38352864, 1.38352864],\n",
       "       [2.2142833 , 1.2142833 , 1.2142833 , 1.2142833 ],\n",
       "       [1.88280282, 0.88280282, 3.88280282, 0.88280282]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(linear_classifer.softmax(np.array([[ 2., -1., -1.,  1.], [ 0.,  1.,  1.,  1.], [ 1.,  2., -1.,  2.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.160204920616996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(-np.log(linear_classifer.softmax(np.array([[ 2., -1., -1.,  1.], [ 0.,  1.,  1.,  1.], [ 1.,  2., -1.,  2.]]))[range(target_index.shape[0]),target_index.T]))/target_index.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24976920411923229"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.sum(-np.log(linear_classifer.softmax(np.array([[ 2., -1., -1.,  1.+delta], [ 0.,  1.,  1.,  1.], [ 1.,  2., -1.,  2.]]))[range(target_index.shape[0]),target_index.T]))/target_index.shape[0])-\n",
    " (np.sum(-np.log(linear_classifer.softmax(np.array([[ 2., -1., -1.,  1.-delta], [ 0.,  1.,  1.,  1.], [ 1.,  2., -1.,  2.]]))[range(target_index.shape[0]),target_index.T]))/target_index.shape[0]))/(2*delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1.  1.]\n",
      " [ 0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[2. 1. 0.]] \n",
      " out= [[4. 1.]] \n",
      " target= [1]\n",
      "softmax= [[0.95257413 0.04742587]]\n",
      "loss= 3.048587351573742\n",
      "grad= [[ 0.95257413 -0.95257413]]\n",
      "grad_w= [[ 1.90514825 -1.90514825]\n",
      " [ 0.95257413 -0.95257413]\n",
      " [ 0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(1, num_features)).astype(np.float)\n",
    "target_index = np.ones(1, dtype=np.int)\n",
    "predictions = np.dot(X, W)\n",
    "print(\"X=\", X, \"\\n out=\", predictions, \"\\n target=\",target_index)\n",
    "pr_max=np.max(predictions,axis=1)\n",
    "for i in range(predictions.shape[0]):\n",
    "    predictions[i]=predictions[i]-pr_max[i]     \n",
    "predictions=np.exp(predictions)\n",
    "pr_sum=np.sum(predictions,axis=1)\n",
    "for i in range(predictions.shape[0]):\n",
    "    predictions[i]=predictions[i]/pr_sum[i]\n",
    "print(\"softmax=\", predictions)\n",
    "loss= np.sum(- np.log(predictions[range(target_index.shape[0]),target_index.T])) / target_index.shape[0]\n",
    "print(\"loss=\", loss)\n",
    "grad=predictions.copy()\n",
    "grad[range(target_index.shape[0]),target_index.T]-=1\n",
    "grad=grad/target_index.shape[0]\n",
    "print(\"grad=\", grad)\n",
    "grad_w=np.dot(X.T,grad)\n",
    "print(\"grad_w=\", grad_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[ 0.  2.  2.]\n",
      " [-1. -1.  2.]] \n",
      " out= [[ 4.  4.]\n",
      " [-1. -2.]] \n",
      " target= [1 1]\n",
      "softmax= [[0.5        0.5       ]\n",
      " [0.73105858 0.26894142]]\n",
      "loss= 1.003204434039084\n",
      "grad= [[ 0.25       -0.25      ]\n",
      " [ 0.36552929 -0.36552929]]\n",
      "grad_w= [[-0.36552929  0.36552929]\n",
      " [ 0.13447071 -0.13447071]\n",
      " [ 1.23105858 -1.23105858]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "predictions = np.dot(X, W)\n",
    "print(\"X=\", X, \"\\n out=\", predictions, \"\\n target=\",target_index)\n",
    "pr_max=np.max(predictions,axis=1)\n",
    "for i in range(predictions.shape[0]):\n",
    "    predictions[i]=predictions[i]-pr_max[i]     \n",
    "predictions=np.exp(predictions)\n",
    "pr_sum=np.sum(predictions,axis=1)\n",
    "for i in range(predictions.shape[0]):\n",
    "    predictions[i]=predictions[i]/pr_sum[i]\n",
    "print(\"softmax=\", predictions)\n",
    "loss= np.sum(- np.log(predictions[range(target_index.shape[0]),target_index.T])) / target_index.shape[0]\n",
    "print(\"loss=\", loss)\n",
    "grad=predictions.copy()\n",
    "grad[range(target_index.shape[0]),target_index.T]-=1\n",
    "grad=grad/target_index.shape[0]\n",
    "print(\"grad=\", grad)\n",
    "grad_w=np.dot(X.T,grad)\n",
    "print(\"grad_w=\", grad_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "$$l2RegLoss = RegularizationStrength \\cdot \\sum_{i,j} w_{i, j}^{2}$$\n",
    "т.е. сумма квадратов всех элементов, умноженная на скорость регуляризации. (В лекции было, что еще и корень от этого)\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "A= np.array([[1, 2], [3, 4]])\n",
    "print(np.sum(A**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "delta=1e-5\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.089999800001\n",
      "0.09000020000100001\n",
      "-0.020000000000575113\n"
     ]
    }
   ],
   "source": [
    "W1=W.copy()\n",
    "W2=W.copy()\n",
    "W1[0,0]+=delta\n",
    "W2[0,0]-=delta\n",
    "print(0.01*np.sum(((W1)**2)))\n",
    "print(0.01*np.sum(((W2)**2)))\n",
    "print(((0.01*np.sum(((W1)**2)))-(0.01*np.sum(((W2)**2))))/(2*delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02,  0.  ],\n",
       "       [ 0.04,  0.04],\n",
       "       [ 0.  ,  0.  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*0.01*W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08975373 -0.03568431  0.08994824 ... -0.03998549  0.09710392\n",
      "   1.        ]\n",
      " [ 0.01612863  0.01921765  0.05465412 ... -0.13410314 -0.10681765\n",
      "   1.        ]\n",
      " [-0.25445961 -0.32980196 -0.36887529 ... -0.20469137 -0.23230784\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.38083451  0.36823725  0.36053647 ...  0.23060275  0.21867255\n",
      "   1.        ]\n",
      " [-0.2426949  -0.22391961 -0.12965961 ... -0.14194627 -0.13034706\n",
      "   1.        ]\n",
      " [ 0.01220706  0.02706078  0.18798745 ...  0.06981843  0.10886863\n",
      "   1.        ]] [9 9 2 ... 4 2 7]\n"
     ]
    }
   ],
   "source": [
    "print(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 3073) количество тренировочных примеров и количество признаков\n"
     ]
    }
   ],
   "source": [
    "num_train = train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "print(train_X.shape, \"количество тренировочных примеров и количество признаков\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(train_y)+1\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.22218266e-05 -4.27792914e-04 -5.31817410e-04 ... -3.46521842e-05\n",
      "   1.13433927e-03 -1.04745548e-04]\n",
      " [-5.25122851e-04  1.91277127e-03 -2.02671962e-03 ...  3.73118915e-04\n",
      "  -3.86472951e-04 -1.15877024e-03]\n",
      " [ 5.66112827e-04 -7.04453450e-04 -1.37793930e-03 ...  1.20089277e-03\n",
      "   6.98398941e-04 -1.71628835e-04]\n",
      " ...\n",
      " [-9.86647835e-05 -6.97203426e-04  1.78094324e-03 ... -1.52311984e-04\n",
      "  -2.73694441e-03 -7.18960098e-04]\n",
      " [ 7.33053535e-04  1.12142976e-04 -9.31122362e-04 ... -7.23149546e-04\n",
      "   1.80309484e-03  5.79680643e-04]\n",
      " [-8.05339206e-04  1.93344482e-05 -1.91584391e-04 ... -8.80196659e-04\n",
      "   1.01721115e-03 -1.02844422e-03]]\n"
     ]
    }
   ],
   "source": [
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3073, 10)\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 8997 8998 8999]\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = np.arange(num_train)\n",
    "print(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5156, 4589,  450, ..., 1840, 2137, 6823])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "sections = np.arange(batch_size, num_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000, 1100,\n",
       "       1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200,\n",
       "       2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300,\n",
       "       3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400,\n",
       "       4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500,\n",
       "       5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600,\n",
       "       6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700,\n",
       "       7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800,\n",
       "       8900])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_indices = np.array_split(shuffled_indices, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11328314, -0.20431176, -0.21985569, ...,  0.0109949 ,\n",
       "         0.01867255,  1.        ],\n",
       "       [ 0.02397176, -0.02784118, -0.08260078, ...,  0.02668118,\n",
       "        -0.00877843,  1.        ],\n",
       "       [ 0.08671686,  0.10549216,  0.0938698 , ...,  0.16785765,\n",
       "         0.06573137,  1.        ],\n",
       "       ...,\n",
       "       [-0.06622431, -0.06313529, -0.12181647, ..., -0.00861294,\n",
       "        -0.03622941,  1.        ],\n",
       "       [ 0.12985412,  0.16823725,  0.15269333, ..., -0.07135804,\n",
       "        -0.02838627,  1.        ],\n",
       "       [ 0.47887373,  0.50941373,  0.52132078, ...,  0.52472039,\n",
       "         0.50886863,  1.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[batches_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16],[17,18,19,20],[21,22,23,24],[25,26,27,28],\n",
    "                  [29,1,5,6],[7,18,19,15]])\n",
    "train_y=np.array([0,0,1,1,1,2,2,0,1])\n",
    "batch_size=3\n",
    "num_train = train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00066498  0.00150533  0.00031802]\n",
      " [-0.00120618  0.00064049  0.00081162]\n",
      " [-0.00070044  0.00106882  0.00045119]\n",
      " [-0.00199274 -0.00058798 -0.0003363 ]]\n"
     ]
    }
   ],
   "source": [
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "epochs=1\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "sections = np.arange(batch_size, num_train, batch_size)\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2, 4, 7]), array([1, 3, 5]), array([8, 6, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(batches_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1=train_X[batches_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 10 11 12]\n",
      " [17 18 19 20]\n",
      " [29  1  5  6]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "train_y1=train_y[batches_indices[0]]\n",
    "print(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,grad=linear_classifer.linear_softmax(train_x1, W,train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss,l2_grad=linear_classifer.l2_regularization(W,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history.append(loss+l2_loss)\n",
    "W-=learning_rate*(grad+l2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00066536,  0.00150557,  0.00031741],\n",
       "       [-0.00120645,  0.00064109,  0.00081129],\n",
       "       [-0.00070064,  0.00106941,  0.0004508 ],\n",
       "       [-0.00199295, -0.00058735, -0.00033673]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0817116530390467, 1.0874791740342056] \n",
      " [[ 0.00066512  0.00150555  0.00031767]\n",
      " [-0.00120669  0.00064107  0.00081155]\n",
      " [-0.00070088  0.00106939  0.00045105]\n",
      " [-0.00199318 -0.00058737 -0.00033647]]\n"
     ]
    }
   ],
   "source": [
    "train_x1=train_X[batches_indices[1]]\n",
    "train_y1=train_y[batches_indices[1]]\n",
    "loss,grad=linear_classifer.linear_softmax(train_x1, W,train_y1)\n",
    "l2_loss,l2_grad=linear_classifer.l2_regularization(W,1e-5)\n",
    "loss_history.append(loss+l2_loss)\n",
    "W-=learning_rate*(grad+l2_grad)\n",
    "print(loss_history, \"\\n\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0817116530390467, 1.0874791740342056, 1.0813972454916663] \n",
      " [[ 0.00066481  0.0015054   0.00031813]\n",
      " [-0.00120709  0.00064114  0.00081189]\n",
      " [-0.00070128  0.00106946  0.0004514 ]\n",
      " [-0.00199353 -0.00058742 -0.00033607]]\n"
     ]
    }
   ],
   "source": [
    "train_x1=train_X[batches_indices[2]]\n",
    "train_y1=train_y[batches_indices[2]]\n",
    "loss,grad=linear_classifer.linear_softmax(train_x1, W,train_y1)\n",
    "l2_loss,l2_grad=linear_classifer.l2_regularization(W,1e-5)\n",
    "loss_history.append(loss+l2_loss)\n",
    "W-=learning_rate*(grad+l2_grad)\n",
    "print(loss_history, \"\\n\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0006902   0.00137243 -0.00038609]\n",
      " [ 0.00017512  0.00031943  0.00084614]\n",
      " [-0.00153243  0.00150351  0.00023486]\n",
      " [ 0.00130612  0.00105529 -0.00213776]\n",
      " [ 0.00033364  0.00049145 -0.00021704]] \n",
      " [[0.33344317 0.33674866 0.32980817]\n",
      " [0.33263893 0.34071726 0.32664381]\n",
      " [0.33169238 0.34461903 0.32368859]\n",
      " [0.331257   0.34929444 0.31944856]\n",
      " [0.33156428 0.35436692 0.3140688 ]\n",
      " [0.32983928 0.35849911 0.31166161]\n",
      " [0.32958954 0.36437345 0.30603701]\n",
      " [0.33455324 0.3465329  0.31891386]\n",
      " [0.32833647 0.34969744 0.32196609]\n",
      " [0.32754169 0.34946502 0.32299329]]\n",
      "Epoch 0, loss: 2.877688\n",
      "Epoch 10, loss: 7.886944\n",
      "Epoch 20, loss: 1.603049\n",
      "Epoch 30, loss: 3.335328\n",
      "Epoch 40, loss: 3.240603\n",
      "Epoch 50, loss: 1.203495\n",
      "Epoch 60, loss: 4.440196\n",
      "Epoch 70, loss: 3.403137\n",
      "Epoch 80, loss: 1.265123\n",
      "Epoch 90, loss: 1.921924\n",
      "[[ 0.32127419 -0.52240198  0.20280425]\n",
      " [-0.31564373  0.31012976  0.00685459]\n",
      " [-0.04814332  0.1506856  -0.10233635]\n",
      " [ 0.22490969 -0.22359221 -0.00109384]\n",
      " [-0.25011428  0.0046511   0.24607121]] \n",
      " [[1.04301573e-01 1.69264620e-01 7.26433806e-01]\n",
      " [3.27444402e-01 4.91885905e-02 6.23367008e-01]\n",
      " [4.59279205e-04 2.01800216e-02 9.79360699e-01]\n",
      " [1.66397787e-04 8.92818355e-04 9.98940784e-01]\n",
      " [1.07682628e-03 2.52698216e-06 9.98920647e-01]\n",
      " [4.19038414e-06 7.66056327e-06 9.99988149e-01]\n",
      " [6.01483119e-06 5.90694280e-08 9.99993926e-01]\n",
      " [4.44791350e-01 4.61004477e-11 5.55208650e-01]\n",
      " [1.68531044e-03 3.08826440e-01 6.89488250e-01]\n",
      " [5.34789302e-06 8.51536751e-01 1.48457901e-01]]\n"
     ]
    }
   ],
   "source": [
    "train_X=np.array([[1,2,3,4,5],[5,6,7,8,3],[9,18,10,11,12],[13,20,14,15,16],[17,13,18,19,20],[21,29,22,23,24],[25,26,27,28,29],\n",
    "                  [29,1,5,6,10],[7,18,19,15,11],[7,28,19,15,13]])\n",
    "train_y=np.array([0,0,1,1,1,2,2,0,1,1])\n",
    "batch_size=5\n",
    "learning_rate=1e-2\n",
    "num_train = train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "print(W, \"\\n\", linear_classifer.softmax(np.dot(train_X, W)))\n",
    "loss_history = []\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    shuffled_indices = np.arange(num_train)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    sections = np.arange(batch_size, num_train, batch_size)\n",
    "    batches_indices = np.array_split(shuffled_indices, sections)\n",
    "    for ind in range(len(batches_indices)):\n",
    "        batch_train_x=train_X[batches_indices[ind]]\n",
    "        batch_train_y=train_y[batches_indices[ind]]\n",
    "        loss,grad=linear_classifer.linear_softmax(batch_train_x, W,batch_train_y)\n",
    "        l2_loss,l2_grad=linear_classifer.l2_regularization(W,1e-5)\n",
    "        loss_history.append(loss+l2_loss)\n",
    "        W-=learning_rate*(grad+l2_grad)\n",
    "        #print(loss_history, \"\\n\",W)\n",
    "    if epoch%10 ==0:\n",
    "        print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
    "print(W,\"\\n\", linear_classifer.softmax(np.dot(train_X, W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32127419 -0.52240198  0.20280425]\n",
      " [-0.31564373  0.31012976  0.00685459]\n",
      " [-0.04814332  0.1506856  -0.10233635]\n",
      " [ 0.22490969 -0.22359221 -0.00109384]\n",
      " [-0.25011428  0.0046511   0.24607121]]\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04301573e-01, 1.69264620e-01, 7.26433806e-01],\n",
       "       [3.27444402e-01, 4.91885905e-02, 6.23367008e-01],\n",
       "       [4.59279205e-04, 2.01800216e-02, 9.79360699e-01],\n",
       "       [1.66397787e-04, 8.92818355e-04, 9.98940784e-01],\n",
       "       [1.07682628e-03, 2.52698216e-06, 9.98920647e-01],\n",
       "       [4.19038414e-06, 7.66056327e-06, 9.99988149e-01],\n",
       "       [6.01483119e-06, 5.90694280e-08, 9.99993926e-01],\n",
       "       [4.44791350e-01, 4.61004477e-11, 5.55208650e-01],\n",
       "       [1.68531044e-03, 3.08826440e-01, 6.89488250e-01],\n",
       "       [5.34789302e-06, 8.51536751e-01, 1.48457901e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax(np.dot(train_X, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=np.array([[1,0,0],[2,-1,0],[1,1,-2],[3,0,-1],\n",
    "                  [4,3,-5],[2,-2,0],[5,-3,-2],[1,3,-2],\n",
    "                  [3,-1,-1],[3,3,-5],[2,-2,2],[6,-3,-2]])\n",
    "train_y=np.array([1,1,0,2,2,0,0,2,1,1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 3 3\n"
     ]
    }
   ],
   "source": [
    "num_train = train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "print(num_train,num_features,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.36723628e-06 -9.76123020e-04  9.05553334e-04]\n",
      " [-8.05571259e-04  1.66621841e-03 -1.34615682e-03]\n",
      " [-3.04817577e-04 -2.48898894e-04 -1.11799789e-03]]\n"
     ]
    }
   ],
   "source": [
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33334316, 0.33301481, 0.33364203],\n",
       "       [0.33356652, 0.33208785, 0.33434563],\n",
       "       [0.33296049, 0.33341845, 0.33362106],\n",
       "       [0.33327778, 0.33227533, 0.33444688],\n",
       "       [0.33230915, 0.33337218, 0.33431866],\n",
       "       [0.33377982, 0.33147984, 0.33474033],\n",
       "       [0.33384728, 0.32971512, 0.3364376 ],\n",
       "       [0.33253091, 0.3346385 , 0.33283059],\n",
       "       [0.33349104, 0.33166712, 0.33484184],\n",
       "       [0.33229944, 0.33369113, 0.33400942],\n",
       "       [0.3339491 , 0.33168505, 0.33436585],\n",
       "       [0.333855  , 0.32939796, 0.33674704]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.softmax(np.dot(train_X, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "batch_size=4\n",
    "for epoch in range(epochs):\n",
    "    shuffled_indices = np.arange(num_train)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    sections = np.arange(batch_size, num_train, batch_size)\n",
    "    batches_indices = np.array_split(shuffled_indices, sections)\n",
    "    for ind in range(len(batches_indices)):\n",
    "        batch_train_x=train_X[batches_indices[ind]]\n",
    "        batch_train_y=train_y[batches_indices[ind]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.76116885e-01, 2.11941558e-01, 2.11941558e-01],\n",
       "       [5.76116885e-01, 2.11941558e-01, 2.11941558e-01],\n",
       "       [7.86986042e-01, 1.06506979e-01, 1.06506979e-01],\n",
       "       [9.09442999e-01, 4.52785007e-02, 4.52785007e-02],\n",
       "       [9.98179556e-01, 9.10221936e-04, 9.10221936e-04],\n",
       "       [3.33333333e-01, 3.33333333e-01, 3.33333333e-01],\n",
       "       [7.86986042e-01, 1.06506979e-01, 1.06506979e-01],\n",
       "       [9.64663156e-01, 1.76684220e-02, 1.76684220e-02],\n",
       "       [7.86986042e-01, 1.06506979e-01, 1.06506979e-01],\n",
       "       [9.95066951e-01, 2.46652437e-03, 2.46652437e-03],\n",
       "       [3.33333333e-01, 3.33333333e-01, 3.33333333e-01],\n",
       "       [9.09442999e-01, 4.52785007e-02, 4.52785007e-02]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ist=np.array([[1,0,0],[1,0,0],[1,1,1]])\n",
    "linear_classifer.softmax(np.dot(train_X, W_ist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
